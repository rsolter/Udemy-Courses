Introduction to Machine Learning Notes


Supervised Learning Overview:
	Supervised Learning - algorithms are trained using labeled examples
	Training Data - Used to train model parameters
	Validation Data - Used to determine what the model hyper-parameters to adjust
	Test Data - Used to get some final performance metric

Evaluating Performance - Classification
	Classification Error Metrics - all measurement variations of correct/incorrect
	In the real world, not all correct/incorrect matches hold equal value, hence the multiple forms of evaluation
	
	Types of Evaluation:
	
		Accuracy - Number of correction predictions/total number of predictions
			 - Useful when target classes our well balanced,
		Recall (sensitivity/true positive rate) - Ability of a model to find all the relevant cases within a dataset
			- Number of true positives / (number of true positives + number of false negatives)
		Precision - Ability of a model to identify only the relevant data points
			  - Number of true positives/(number of true positives + number of false positives)
		F1 - The harmonic mean of precision and recall taking both metrics into account:
		   - F1 = 2*[(precision*recall)/(precision+recall)]
		   - Harmonic mean is used because it punishes extreme values (a classifier with precision of 1, recall of 0 would have an F1 score of 0)

	Confusion Matrix pulls all of this together to also include
		Accuracy
		Positive Predictive Value Precision
		False Discovery Rate
		False Omisssion Rate
		Negative Predictive Value
		Positive Likelihood Ratio
		Negative Likelihood Ratio

	Type I error - False positive
	Type II error - False negative

Evaluation Performance - Regression
	Regression is a task where a model attempts to prediction continuous values
	Mean Absolute Error -
		- Will not punish large errors, outliers
	Mean Squared Error -
		- Errors are reported in units squared
	Root Mean Square Error (RMSE) -
		- Accounts for deficiencies in MAE, MSE
		- Context determines whether or not the RMSE is good or not
		- Compare the error metric to the average value of the label in your dataset



Linear Regression Theory
	Regression - The propensity of values to regress or trend towards the average
	Least Squares Method - Optimize model on the vertical distance between all observed points and distance to the line
	RMSE - Most popular method is Root-mean, squared errors	which accounts the size of errors and reports error in unit terms

Cross Validation

Bias/Variance Trade-off
	Fundamental topic for understanding model Evaluation
	Bias/Variance trade-off is the point at which we are just adding noise by adding model complexity
	The training error may be going down, but the test error is going up
	The model after the bias trade-off begins to over-fit


Logistic Regression
	Based upon sigmoid functions which return values between 0 and 1
	This output/probability is used to assign a predicted class depending on the cut-off point
	Determining a cut-off point. Is it always 0.5?
	Model evaluation using a confusion matrix (TP, TN, FP (Type I Error), FN (Type II Error))
	Overall Accuracy - (TP + TN) / N
	Misclassification Rate - (FP + FN)/ N
	Missing value Imputation - naive or by category (ex: imputing age by traveler class)
	Creating dummy variables (pd.get_dummies) from categorical variables. Ensuring encoding doesn't lead to multicollinearity, linear dependence
			

K Nearest Neighbors
	Classification Training algorithm:
	- Store all teh data
	Classification Prediction Algorith:
	- Calculate the distance from x to all points in your data
	- Sort the points in your data by increasing distance from x
	- Predict the majority label of the 'k' closest points

	Choosing a k will affect what class a new point is assigned to.
	- e.g. k=3 -> choose the 3 closest points, and predict teh label based on the majority

	Pros:
	- Simple
	- Training is trivial (just sorting your data)
	- Works with any number of classes
	- Easy to add mroe data
	- Few parameters:
		- k
		- Distance metric 

	Cons:
	- High prediction cost (worse for larger datasets)
	- Not good with high dimensional data
	- Categorical features don't work well


	Other steps:
	- distance-based so, requires scaling variables



Decision Trees and Random Forests
	Root - The node that performs the first split
	Leaves - Terminal nodes that predict the outcome
	Best splits are determined on the basis of entropy and information gain
	Plainly speaking, best splits are based upon maximizing information gain 

	Decision trees on their own do not have much predictive power as they are subject to the data sampled
	Introducing bagging -->
	Bootstrapped samples --> random samples with replacement
	Random forests are an improvement on this method as they use many trees with a random sample of features chosen
		- A new random sample of features is chosen for every single tree at every single split
		- For classification, m (random sample of features) is typically chosen to be the square root of p (# of original features)
	

Support Vector Machines


K Means Clustering


Principal Component Analysis


Recommender Systems

	Extra reading: 'Recommender Systems' by Jannach and Zanker 
	Advanced recommender systems require a heavy lienar algebra background
	Two types of systems:
		Collaborative Filtering - based on knowledge of users attitudes. More commonly used (better results, can do feature learning on its own)
			Memory-based: computes cosine similarity
			Model-based: uses single value decomposition (SVD)
		Content-Based - focuses on attributes of the items and gives recs based off on similarity between items


Natural Language Processing


Neural Nets and Deep Learning


Big Data and Spark with Python



