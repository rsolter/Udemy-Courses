Introduction to Machine Learning Notes


Supervised Learning Overview:
	Supervised Learning - algorithms are trained using labeled examples
	Training Data - Used to train model parameters
	Validation Data - Used to determine what the model hyper-parameters to adjust
	Test Data - Used to get some final performance metric

Evaluating Performance - Classification
	Classification Error Metrics - all measurement variations of correct/incorrect
	In the real world, not all correct/incorrect matches hold equal value, hence the multiple forms of evaluation

	Types of Evaluation:

		Accuracy - Number of correction predictions/total number of predictions
			 - Useful when target classes our well balanced,
		Recall (sensitivity/true positive rate) - Ability of a model to find all the relevant cases within a dataset
			- Number of true positives / (number of true positives + number of false negatives)
		Precision - Ability of a model to identify only the relevant data points
			  - Number of true positives/(number of true positives + number of false positives)
		F1 - The harmonic mean of precision and recall taking both metrics into account:
		   - F1 = 2*[(precision*recall)/(precision+recall)]
		   - Harmonic mean is used because it punishes extreme values (a classifier with precision of 1, recall of 0 would have an F1 score of 0)

	Confusion Matrix pulls all of this together to also include
		Accuracy
		Positive Predictive Value Precision
		False Discovery Rate
		False Omisssion Rate
		Negative Predictive Value
		Positive Likelihood Ratio
		Negative Likelihood Ratio

	Type I error - False positive
	Type II error - False negative

Evaluation Performance - Regression
	Regression is a task where a model attempts to prediction continuous values
	Mean Absolute Error -
		- Will not punish large errors, outliers
	Mean Squared Error -
		- Errors are reported in units squared
	Root Mean Square Error (RMSE) -
		- Accounts for deficiencies in MAE, MSE
		- Context determines whether or not the RMSE is good or not
		- Compare the error metric to the average value of the label in your dataset



Linear Regression Theory
	Regression - The propensity of values to regress or trend towards the average
	Least Squares Method - Optimize model on the vertical distance between all observed points and distance to the line
	RMSE - Most popular method is Root-mean, squared errors	which accounts the size of errors and reports error in unit terms

Cross Validation

Bias/Variance Trade-off
	Fundamental topic for understanding model Evaluation
	Bias/Variance trade-off is the point at which we are just adding noise by adding model complexity
	The training error may be going down, but the test error is going up
	The model after the bias trade-off begins to over-fit


Logistic Regression
	Based upon sigmoid functions which return values between 0 and 1
	This output/probability is used to assign a predicted class depending on the cut-off point
	Determining a cut-off point. Is it always 0.5?
	Model evaluation using a confusion matrix (TP, TN, FP (Type I Error), FN (Type II Error))
	Overall Accuracy - (TP + TN) / N
	Misclassification Rate - (FP + FN)/ N
	Missing value Imputation - naive or by category (ex: imputing age by traveler class)
	Creating dummy variables (pd.get_dummies) from categorical variables. Ensuring encoding doesn't lead to multicollinearity, linear dependence


K Nearest Neighbors
	Classification Training algorithm:
	- Store all teh data
	Classification Prediction Algorith:
	- Calculate the distance from x to all points in your data
	- Sort the points in your data by increasing distance from x
	- Predict the majority label of the 'k' closest points

	Choosing a k will affect what class a new point is assigned to.
	- e.g. k=3 -> choose the 3 closest points, and predict teh label based on the majority

	Pros:
	- Simple
	- Training is trivial (just sorting your data)
	- Works with any number of classes
	- Easy to add mroe data
	- Few parameters:
		- k
		- Distance metric

	Cons:
	- High prediction cost (worse for larger datasets)
	- Not good with high dimensional data
	- Categorical features don't work well


	Other steps:
	- distance-based so, requires scaling variables



Decision Trees and Random Forests
	Root - The node that performs the first split
	Leaves - Terminal nodes that predict the outcome
	Best splits are determined on the basis of entropy and information gain
	Plainly speaking, best splits are based upon maximizing information gain

	Decision trees on their own do not have much predictive power as they are subject to the data sampled
	Introducing bagging -->
	Bootstrapped samples --> random samples with replacement
	Random forests are an improvement on this method as they use many trees with a random sample of features chosen
		- A new random sample of features is chosen for every single tree at every single split
		- For classification, m (random sample of features) is typically chosen to be the square root of p (# of original features)


Support Vector Machines
	Non-probabilistic binary linear classifier
	SVM model is a representation of the examples as points in space, mapped into two separate categories and separated by a gap
	New examples are mapped into the same space and predicted to belong to a category based on which side of the gap they fall on
	The gap is represented by a hyperplane
	Hyperplane is chosen based upon maximizing the margin between the classes
	Support vectors are those dividing hyperplanes which touch observations in either class. The limits of the gap
	Kernel trick - viewing data in +1 higher dimension allows for a separating line
	Tuned with grid search
		C
		Gamma
	See more here - https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html

K Means Clustering
	Goal is to divide data into distrincy groups such that observations within each group are similar
	Examples
	- Cluster similar documents
	- Cluster customers based on Features
	- Market segmentation
	- Identify similar physical groups

	K Means algorithm:
	- Choose a numebr of clusters "K"
	- Randomly assign each point to a cluster
	- Until clusters stop changing, repeat the following:
		- For each cluster, compute the clsuter centroid by taking the mean vector of poitns in the cluster
		- Assign each data point to the cluster for which the centroid is closest

	Chosing a K value: One method is using the elbow method.
	- Compute the SSE for some values of K (sum of squared distance between each member of the cluster and its centroid)
	- Re-compute for multiple values of K
	- SSE will necessairily decrease with larger values of K
	- Plot k against SSE and look for elbow in chart




Principal Component Analysis (PCA)

First lecture---
Unsupervised learning technique used to understand the interrelations among a set of variables in order to identify the underlying structure of those variables.
It is also sometimes known as a general factor analysis
Where regression determines a line of best fit to a dataset, factor analysis determines several orthogoal lines of best fit to the data set.
n-Dimensional space is the variable sample space. There are as many dimensions as there are variables.
<<Good visualizations of this in lecture>>
Used on a dataset with a large number of variables, we can compress the amount of explained variation to just a few components.
The challenge is interpreting those components
Requires standardization of data (from sklearn.preprocessing import StandardScaler)
Used for analysis of data, not for creating a model

Second lecture---
PCA allows us to select the number of components we want to keep when creating PCA object
The components correspond to (linear) combinations of the original features
(https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)
https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another
How to fit this in: PCA can be used to come up with features that are then fed into a classifier for example



Recommender Systems

	Extra reading: 'Recommender Systems' by Jannach and Zanker
	Advanced recommender systems require a heavy lienar algebra background
	Two types of systems:
		Collaborative Filtering - based on knowledge of users attitudes. More commonly used (better results, can do feature learning on its own)
			Memory-based: computes cosine similarity
			Model-based: uses single value decomposition (SVD)
		Content-Based - focuses on attributes of the items and gives recs based off on similarity between items


Natural Language Processing
	Bag of words -> vectorizing documents into vectors with word counts, comparing vector similarity using cosine similarity

	TF-IDF (Term Frequency - Inverse Document Frequency)
		Term Frequency: TF(d,t) Number of occurrences of term t in document d
		IDF(t) = log(D/t) where D is total number of Documents, t= number of documents with the terms
		TF-IDF : W = tf * log(N/df)




Neural Nets and Deep Learning


Big Data and Spark with Python
